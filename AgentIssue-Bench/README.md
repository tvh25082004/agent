# AGENTISSUE-BENCH

[![Paper](https://img.shields.io/badge/Paper-ARXIV%3A2505.20749-B31B1B)](https://arxiv.org/pdf/2505.20749)&nbsp;&nbsp;
[![Leaderboard](https://img.shields.io/badge/View-Leaderboard-blue)](https://alfin06.github.io/AgentIssue-Bench-Leaderboard/)

***AGENTISSUE-BENCH*** is the first reproducible issue resolution benchmark focused on real-world agent system issues. It is designed to evaluate the efficacy of state-of-the-art software engineering (SE) agents in resolving these issues.

## ğŸ—“ï¸ Updates

- 2025-05: Initial benchmark release

## ğŸ“š Benchmark Dataset

Through a multi-step filtering processâ€”including failure reproduction, patch reproduction, and non-flakiness verificationâ€”we collect 50 reproducible agents issues, which form ***AGENTISSUE-BENCH***.

Each issue is containerized as a Docker image and hosted on Docker Hub:
ğŸ”— **[Docker Hub Repository](https://hub.docker.com/r/alfin06/agentissue-bench/tags)**

To retrieve the images for all issues, run:
```
$ python pull_images.py
```
To pull a specific image by tag, use:
```
$ python pull_images.py --tag <tag>
```

To remove all pulled Docker images and containers, run:
```
$ python remove_images.py
```
To remove a specific image and container by tag:
```
$ python remove_images.py --tag <tag>
```
To test the issues in ***AGENTISSUE-BENCH***:
```
$ python test_agentissue_bench.py
```

## ğŸ“Š Results

### Overall Resoultion Rate
The following figure shows the distribution of ***AgentIssue-Bench***:
<img src="output\images\pie.png" alt="pie" />

The following figure shows the resolution rate of AgentIssue-Bench v.s. traditional software issues:
<img src="output\images\bar.png" alt="bar" />

The following table presents the overall results of SE agents on AgentIssue-Bench:
<img src="output\images\table_results.png" alt="table_results" />

## ğŸ§ª Patch Evaluation

To evaluate generated patches in ***AGENTISSUE-BENCH***:

1. Create a directory named `Patches`:
```
mkdir Patches
```
2. Place your patch files inside subdirectories named by tag:
```
Patches/{tag_name}/your_patch_files.patch
```
3. Run the evaluation script:
```
python eval_patches.py
```
4. You can see the result in `patch_eval.log`

<!-- ## ğŸ”§ Patch Generation

We evaluate the capabilities of 3 state-of-the-art SE agents on ***AGENTISSUE-BENCH***, collecting the patches they generate to resolve real-world agent issues.

### ğŸ› ï¸ Setup Instructions

### 1. Clone the Repository

```
$ git clone https://github.com/alfin06/AgentIssue-Bench.git
```

### 2. Run studied SE Agents

*Note: please download the repo folder from ğŸ”—[Repo Link](https://drive.google.com/file/d/1obN3y2HYDsHfvr8hi0rymldgkjel5HNv/view?usp=sharing) . Extract the file and store the repo/ folder in Agentless' root directory and AutoCodeRover's root directory for patch generation.*

**Agentless**

```
$ cd Agentless
$ conda create -n agentless python=3.12
$ conda activate agentless
$ chmod +x run_agentless.sh
$ ./run_agentless.sh
```

**AutoCodeRover**

```
$ cd auto-code-rover
$ conda create -n auto-code-rover python=3.12
$ conda activate auto-code-rover
$ python run_autocoderover.py
```

**SWE-agent**

```
$ cd SWE-agent
$ conda create -n swe_agent python=3.12
$ conda activate swe_agent
$ chmod +x gen_patches_all.sh
$ ./gen_patches_all.sh
``` -->

## ğŸ“ Generated Patches

The `Generated Patches` directory contains all patches generated by our evaluation of different SE agents and Large Language Models (LLMs). The patches are organized as follows:

```
Generated Patches/
â”œâ”€â”€ swe-agent/         # Patches generated by SWE-agent
â”œâ”€â”€ Agentless/         # Patches generated by Agentless
â””â”€â”€ Auto-code-rover/   # Patches generated by Auto-code-rover
```

Each agent directory contains patches generated using two state-of-the-art LLMs:

- **claude-3-5-sonnet-20241022**
- **gpt-4o**
