{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Manual Analysis of AgentIssue-Bench: Why Do Agents Fail?\n",
        "\n",
        "This notebook provides a human-perspective analysis of agent failures on real-world agent system bugs, examining logs from Auto-Code-Rover to understand failure patterns.\n",
        "\n",
        "**Analysis Framework:**\n",
        "1. Bug Reproduction\n",
        "2. Bug Localization\n",
        "3. Fix Generation\n",
        "4. Fix Validation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Data Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "# Load issue data\n",
        "with open('auto-code-rover/agent_issue.json', 'r') as f:\n",
        "    issues = json.load(f)\n",
        "\n",
        "print(f\"Total issues in dataset: {len(issues)}\")\n",
        "print(f\"\\nFirst issue example:\")\n",
        "print(f\"ID: {issues[0]['instance_id']}\")\n",
        "print(f\"Title: {issues[0]['title']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Issue Categorization by Type\n",
        "\n",
        "Let's categorize issues by their root cause to understand the dataset composition.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Categorize issues\n",
        "categories = {\n",
        "    'dependency': [],\n",
        "    'configuration': [],\n",
        "    'logic_error': [],\n",
        "    'type_handling': [],\n",
        "    'state_management': []\n",
        "}\n",
        "\n",
        "# Simple keyword-based categorization\n",
        "for issue in issues[:20]:  # Analyze first 20\n",
        "    title = issue['title'].lower()\n",
        "    problem = issue['problem_statement'].lower()\n",
        "    \n",
        "    if 'modulenotfounderror' in problem or 'no module named' in problem:\n",
        "        categories['dependency'].append(issue['instance_id'])\n",
        "    elif 'config' in problem or 'yaml' in problem or 'json' in problem:\n",
        "        categories['configuration'].append(issue['instance_id'])\n",
        "    elif 'keyerror' in problem or 'attributeerror' in problem:\n",
        "        categories['logic_error'].append(issue['instance_id'])\n",
        "    elif 'type' in problem or 'boolean' in problem:\n",
        "        categories['type_handling'].append(issue['instance_id'])\n",
        "    else:\n",
        "        categories['state_management'].append(issue['instance_id'])\n",
        "\n",
        "# Display results\n",
        "for cat, items in categories.items():\n",
        "    print(f\"{cat}: {len(items)} issues\")\n",
        "    if items:\n",
        "        print(f\"  Examples: {items[:3]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Deep Dive: Case Study - Dependency Issue (lagent-279)\n",
        "\n",
        "**Issue:** Missing `tenacity` dependency\n",
        "\n",
        "**From human perspective, this should be EASY to fix:**\n",
        "- Clear error message pointing to missing module\n",
        "- Solution: Add `tenacity` to requirements.txt\n",
        "- 1-line fix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Examine lagent-279 issue\n",
        "lagent_issue = [i for i in issues if 'lagent-279' in i['instance_id']][0]\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"CASE STUDY: lagent__lagent-279\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nTitle: {lagent_issue['title']}\")\n",
        "print(f\"\\nError (excerpt):\")\n",
        "print(lagent_issue['problem_statement'][:300])\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"HUMAN ANALYSIS:\")\n",
        "print(\"=\"*60)\n",
        "print(\"âœ… Clear reproduction: Just run the command\")\n",
        "print(\"âœ… Easy localization: Error points directly to missing module\")  \n",
        "print(\"âœ… Simple fix: Add 'tenacity' to requirements/runtime.txt\")\n",
        "print(\"âœ… Easy validation: Run command again\")\n",
        "print(\"\\nâ“ Why agents might struggle:\")\n",
        "print(\"   - Need to understand project dependency management\")\n",
        "print(\"   - Must know which requirements file to modify\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Agent Failure Analysis: Patch Outcomes\n",
        "\n",
        "Now let's examine actual agent outcomes to see success rates.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count different patch outcomes\n",
        "patch_outcomes = {\n",
        "    'applicable_patch': 0,\n",
        "    'raw_patch_but_unmatched': 0,\n",
        "    'raw_patch_but_unparsed': 0,\n",
        "    'matched_but_empty': 0\n",
        "}\n",
        "\n",
        "results_dir = Path('auto-code-rover/results/acr-run-1')\n",
        "for outcome_dir in results_dir.iterdir():\n",
        "    if outcome_dir.is_dir() and outcome_dir.name in patch_outcomes:\n",
        "        count = len(list(outcome_dir.iterdir()))\n",
        "        patch_outcomes[outcome_dir.name] = count\n",
        "\n",
        "print(\"Agent Patch Generation Outcomes:\")\n",
        "print(\"=\"*60)\n",
        "for outcome, count in patch_outcomes.items():\n",
        "    print(f\"{outcome:30s}: {count:3d} issues\")\n",
        "\n",
        "total = sum(patch_outcomes.values())\n",
        "success_rate = (patch_outcomes['applicable_patch'] / total * 100) if total > 0 else 0\n",
        "print(f\"\\nSuccess rate: {success_rate:.1f}%\")\n",
        "print(f\"\\nðŸ’¡ Insight: Only ~{success_rate:.0f}% of patches are directly applicable!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
